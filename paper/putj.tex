% -----------------------------------------------
% Template for ISMIR Papers
% 2015 version, based on previous ISMIR templates
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir}
\usepackage{url}
\usepackage{cleveref}
\usepackage{cite}
\usepackage{brian}
\usepackage{graphicx}
\usepackage{booktabs}

% Title.
% ------
\title{Musical data augmentation}


% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Predictive models for music annotation tasks are practically limited by a paucity of
well-annotated training data.
In the broader context of large-scale machine learning, the concept of ``data
augmentation'' --- supplementing a training set with carefully perturbed samples ---
has emerged as an important component of robust systems.
In this work, we develop a general software framework for augmenting annotated
musical datasets, which will allow practitioners to easily expand training sets
with musically motivated perturbations of both audio and annotations.
As a proof of concept, we investigate the effects of data augmentation on
the task of recognizing instruments in mixed signals.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}

% Main points to make:
%   1. music is complex, and needs big models
%       a. big models need big data
%       b. big (annotated) data is hard to come by

Musical audio signals contain a wealth of rich, complex, and highly structured
information.  The primary goal of content-based music information retrieval (MIR) is to
analyze, extract, and summarize music recordings in a human-friendly
format, such as semantic tags, chord and melody annotations, or structural boundary
estimations.  To adequately capture and characterize the vast complexity of musical
recordings seems to require large, flexible models with many parameters.
In short, complex data require complex models.
By the same token, estimating the parameters of complex statistical models often requires
a large number of samples: big models require big data.

Within the past few years, this same phenomenon of model complexity has been observed
in the computer vision literature.  Currently, the best-performing models for recognition
of objects in images exploit two fundamental properties to overcome the difficulty of
fitting large, complex models: access to large quantities of annotated data, and
identification of label-invariant transformations~\cite{krizhevsky2012imagenet}.
The benefits of large training collections are obvious, and unfortunately, difficult to
carry over to most musical annotation tasks due to the complexity of the label space and
need for expert annotators.  However, the idea of generating perturbations of a training
set --- known as \emph{data augmentation} --- can be readily adapted to musical tasks.

%   2. in images:
%       a. data augmentation has proven useful in vision.
%       b. general idea: perturb your data such that the features change, but the labels
%       don't
%       c. rotations, reflections, contrast normalization, affine transformations
Conceptually, data augmentation consists of the application of one or more deformations to
a collection of (annotated) training samples.
Data augmentation is motivated by the observation that a learning algorithm should be less
susceptible to over-fitting and spurious correlations if it is provided with many
observations of an instance which have been perturbed in ways which do not affect its
label.
Some concrete examples of deformations drawn from computer vision include translation,
rotations, and reflections.  These simple operations are appealing because they typically
do not affect the target class label. An upside-down image of a cat is still contains cat,
although the situation may be more complex for concepts which are not reflection-invariant,
such as in optical character recognition.  Consequently, practitioners must exercise some
caution when applying data augmentation techniques to ensure that the correct invariances
are maintained.

Continuing with the image example, if an image is rotated, then any pixel-wise
label annotations (\eg, bounding boxes) should be modified accordingly.
More generally, deformations apply not only to the observable stimulus,
but its annotations as well.
This observation opens up several interesting possibilities for musical applications, in
which the target concept space typically exhibits a high degree of structure.
As a simple example, time-stretching an audio track should also result in movement of
time-keyed annotations~\cite{mauch2013audio}.
Moreover, time-dependent annotation \emph{values} such as \emph{tempo} can also be adjusted.
As a more complex example,
pitch-shifting a track should induce transpositions of melodic pitch contours,
and if the transposition is sufficiently large, chord labels or symbolic annotations
should also be modified.
To summarize, successful application of data augmentation techniques in MIR may require
substantially more careful implementations and transformation algorithms than are currently
in use in other domains.

%   3. but data augmentation is more difficult in music than in images
%       a. the output space is much more complex than simple tags
%       b. it's not obvious which variations preserve output structure
%       c. simple example: time-stretching will move annotation boundaries
%       d. complex example: pitch-shifting will deform chord labels

\subsection{Our contributions}
% 1. develop a generic framework for synchronously manipulating audio and annotations
% 2. investigate the effects of simple deformations on the problem of musical instrument recognition
In this work, we describe a software architecture for applying data augmentation to music
information retrieval tasks.\footnote{For anonymity purposes, the name of the software
    described here is redacted throughout the text.  This will be changed in the final
draft, and download links will be provided.}
The system is designed to be simple, modular, and
extensible. The design enables practitioners to develop custom deformations, and combine
multiple simple deformations together into pipelines which can generate large volumes of
reliably deformed, annotated music data.  The proposed system is built on top of
JAMS~\cite{humphreyjams}, which provides a simple container for accessing and
transporting multiple annotations for a given track.

As a proof of concept, we apply the proposed data augmentation architecture to the
task of recognizing instruments in mixed signals, and demonstrate that simple
manipulations can yield improvements in accuracy.

\section{Related work}

% 1. it's common to engineer systems to attempt to resolve symmetries in the input,
%       eg, chroma features are engineered to be approximately invariant to timbre and octave
%       some authors suppress the 0th mfcc to get loudness invariance
As practitioners, the first step in developing a solution to some MIR task is often to
design features which discard information which is irrelevant to predicting the target
concept.  For example, chroma features are designed to capture pitch class information
and suppress information derived from timbre, loudness, or octave
height~\cite{muller2011chroma}.  Similarly, many authors interested in modeling timbre
use Mel-frequency cepstral coefficients (MFCCs) and discard the first component to
achieve invariance to loudness~\cite{pampalk2004matlab}.
This approach makes intuitive sense, but it carries certain limitations.
First, it is not necessarily easy to identify all relevant symmetries the
data: if it was easy, the modeling problem would be essentially solved.
Second, even if such properties are easy to identify, it may still be difficult to
engineer appropriately invariant features without discarding potentially useful
information.  For example, 2-D Fourier magnitude coefficients achieve invariance to
time- and pitch-transposition, but also discard phase coherence~\cite{ellis2012large}.

%   but there are some drawbacks:
%       a. it's not easy to identify *all* relevant symmetries
%       b. even if it was, it might not be easy to engineer an invariant feature
%       c. and you might accidentally discard useful information in the process
%
%   alternatively:
%       we can use bigger models
%       learn the appropriate invariances from statistics.
%       but this takes a lot of (annotated) data, which we usually don't have
%
%
As an alternative to custom feature design, some authors advocate learning or optimizing
features directly from the data~\cite{humphrey2012moving}.
Perhaps not surprisingly, this approach typically requires large model architectures, and
consequently requires much larger (annotated) data sets than had previously been
used in MIR research.
Due to the high cost of acquiring annotated musical data, it has so far been difficult to
apply these techniques in most MIR tasks.
Our goal in this work is to ease this burden of sample complexity, and make
data-driven models more accessible to the MIR community.

% 1. augmentation is not new, but it hasn't been done systematically.
%   eg, chroma rotation for key-invariance in chord quality or mode
%   synthetic mixtures of clean signals
%   perturbations of the labels ``target smearing''
%
Specific instances of data augmentation can be found throughout the MIR literature,
though they are not often identified as such, nor are they treated in a unified,
systematic way.  For example, it is common to apply circular rotations to chroma features
in order to achieve key invariance when modeling chord quality~\cite{lee2008acoustic}.
Alternately, synthetic mixtures of monophonic instruments have been used
to generate non-trivial examples when training polyphonic transcription
engines~\cite{kirchhoff2012multi}.
At the other end of the spectrum, some authors leave the audio content unchanged and
only modify labels during training, as exemplified by the \emph{target smearing}
method of Ullrich~\etal\ for training structural boundary
detectors~\cite{ullrich2014boundary}.

% finally, recent studies have investigated stability of models by evaluating on degraded signals:
%   but it's not clear that the degraded signals resemble the distribution of naturally occurring sounds
%   our goal is different: train on degraded signals, and evaluate on unmodified signals
%
%
Finally, recent studies have used degraded signals to evaluate the stability of
existing methods for MIR tasks.
The Audio Degradation Toolbox (ADT) was developed for this purpose, and was used
to measure the impact of naturalistic deformations of audio on several tasks, including
beat tracking, score alignment, and chord recognition~\cite{mauch2013audio}.
Similarly, Sturm and Collins proposed the ``Kiki-Bouba Challenge'' as a way to determine
whether statistical models of musical concepts actually capture the defining
characteristics of a category (\eg, genre), or are over-fitting to spurious
correlations~\cite{sturmkiki}.

In both of the studies cited above, models are fit to unmodified data, and evaluated in
degraded conditions under the control of the experimenter.
Data augmentation provides the converse of this setting: models are fit to degraded data,
and evaluated on unmodified examples.  The distinction between the two approaches is
critical.  The former attempts to measure the robustness of a system under synthetic
conditions, while the latter attempts to improve robustness by \emph{training} under
synthetic conditions. Note that with data augmentation, the evaluation set is left
untouched by the experimenter, so the resulting comparisons are unbiased with respect
to the underlying distribution from which the data are sampled.  While this does not
directly measure robustness of the resulting system, it has still been observed that data
augmentation can be used effectively to improve generalization in
practice~\cite{krizhevsky2012imagenet,he2015delving}.


\section{Data augmentation architecture}

% 1. because of the complex structure of annotations, we need to be careful
%   a. annotations aren't just track-level, but generally time-keyed
%   b. simple deformations can change annotations, such as chord labels or pitch
%   frequencies
%
% 2. why not use the audio degradation toolbox?
%   a. we'd like to be more extensible, support annotation-dependent deformations
%   b. support multiple annotations per-track
%   c. want the ability to embed history within the annotations for reproducibility
%   purposes
%   d. closer integration with python libraries for machine learning (eg theano)
%
% 3. we developed a generic, plugin-oriented architecture for doing musical data augmentation
%   a. hooks into JAMS, and inherits validation/schema.
%   b. also makes it easy to modify all annotations for a given track in one shot.
%   c. allows the developer to register a deformation against different types of data
%       eg, a ``pitch-shift'' deformer implements an audio deformation, pitch annotation modification, and
%       chord/key manipulators
%   e. modules can be chained or skipped in a pipeline, similar to sklearn feature extractors
%       pipelines can be serialized, stored, shared, and reimplemented easily
%   f. full history of modification state is preserved within the output JAMS sandbox, so the results are
%   documented and reproducible
%       this includes all random state
%   g. deformations can be stochastic, and potentially generate infinite streams of randomized data
%       more generally, a deformer object can implement its own state transition logic
%       using python iterators makes this simple, self-contained, and memory-efficient
Our implementation takes inspiration from the Audio Degradation
Toolbox~\cite{mauch2013audio}.  In principle, the ADT can be used directly
for data augmentation simply by applying it to the training set rather than test set.
However, we opted for an independent, Python-based implementation for a variety of
reasons as detailed below.

% 1. object-oriented design
%   simple, reusable components that are easy to modify and extend
%
First, Python enables object-oriented design, allowing for structured,
extensible, and reusable code.  This in turn facilitates a simple interface shared across
all \emph{deformation objects}, and makes it easy for practitioners to combine existing
deformations, or implement new transformations with minimal effort.

% 2. leverage jams as a container
%   transform all annotations for an object at once
%       -> everything stays in sync easily
%   store the transformation properties within the jams sandbox
%       -> reproducibility, provenance
%   the jams sandbox can store the audio and other intermediate features
%       ->
%   allow deformers to apply different transformations to different types of annotations
%       implemented as simple callbacks registered against annotation types or patterns
%

Second, we leverage JAMS objects~\cite{humphreyjams} to contain and transport all
annotations for a given track.
This simplifies the task of maintaining synchronization between audio and annotations,
and also facilitates task-dependent annotation deformations.  Moreover, we can take
advantage of JAMS meta-data fields to provide data provenance and facilitate
reproducibility.

% 3. use familiar design paradigms
%   take inspiration from the Transformer and Pipeline paradigms of scikit-learn
%
Finally, we can borrow familiar software design concepts from the SciKit-Learn
package~\cite{scikit-learn}, such as the notions of \emph{transformers},
\emph{pipelines}, and model serialization.  These simple building blocks allow
practitioners to assemble and deploy complex data augmentation pipelines from small,
conceptually simple components.

% Give a figure illustrating how deformers work
%   deformer
%       generates states as a function of a jam
%       registers callbacks against annotation types
%   pipeline
%       chains deformers together
%   bypass
%       make a deformer optional

In the remainder of this section, we will describe each of these
properties in more detail.  Without loss of generality, we will assume that an
annotation (\eg, instrument activations or semantic tags) is encoded as a collection of
tuples: \emph{(time, duration, value, confidence)}.
Note that instantaneous events can be represented as having zero duration, while
track-level annotations have full-track duration.  The \emph{value} field will depend on
the annotation type, and may encode strings, numeric quantities, or fully structured
objects.

\subsection{Deformation objects}

At the core of our implementation is the concept of a \emph{deformation object}.
A deformation object implements one or more \emph{transformation} methods, each of which
applies to audio data, meta-data, or annotations.  An auxiliary \emph{state} object is used
to share information across method calls, so each transformation method takes as input a
state-data pair $(S, x)$ and returns the suitably transformed audio, meta-data, or
annotation $x'$.  Decoupling a deformation object's instantiation from
its state allows multiple tracks to be processed in parallel, and as detailed in
\Cref{sec:reproducibility}, promotes reproducibility by making state objects re-usable.

It would be inefficient to instantiate a separate deformation object for each
candidate parameterization.
In practice, data augmentation can be performed by sweeping over a range of
parameters, or randomly sample from a distribution over parameters.  To support this
behavior, deformation objects can implement arbitrary state transition logic, and produce
a sequence of state objects through Python's \emph{generator} functionality.

Deformation objects can register transformation functions against the type of an
annotation as described by regular expressions.  For example, the pattern ``\texttt{.*}''
matches all annotation types, while ``\texttt{chord.*}'' matches only chord-type
annotations.
There is no need for these patterns to be unique or disjoint, though care must be taken
to ensure consistent behavior.  In our implementation, deformations are always applied in the
order in which they are registered, and we encourage the convention that patterns be
registered in order of decreasing generality.

The high-level transformation engine is described in
\Cref{alg:transformation}.  For each state $S$, the input data is copied, transformed,
and yielded.  Each transformed object $J'$ can either be saved off to disk, fed directly
as a sample to a learning algorithm, or passed along to another deformation object in a
pipeline for subsequent processing.  When all subsequent processing of $J'$ has
completed, \Cref{alg:transformation} resumes computation at line~10 and proceeds to the
next state.

\begin{algorithm}[t]
\caption{Abstract transformation pseudocode\label{alg:transformation}}
\begin{algorithmic}[1]
    \Require{Deformation object $D$, JAMS object $J$}
    \Ensure{Sequence of transformed JAMS objects $J'$}
    \Function{$D$.transform}{$J$}
    \For{states $S$ in $D$}
        \State{$J' \leftarrow \text{copy}(J)$}
        \State{$J'$.audio $\leftarrow D$.audio($S$, $J'$.audio)}
        \State{$J'$.metadata $\leftarrow D$.metadata($S$, $J'$.metadata)}
        \For{transformations $g$ in $D$}
            \For{annotations $a$ in $J'$ which match $g$}
                \State{$J'.a \leftarrow g(S, a)$}
            \EndFor{}
        \EndFor{}
        \State{$J'$.history $\leftarrow S + J'$.history \label{transform:history}}
        \State{\textbf{yield} $J'$}
    \EndFor{}
    \EndFunction{}
\end{algorithmic}
\end{algorithm}


\subsubsection{Example: time-stretching}
To illustrate the deformation object interface, consider a \emph{time-stretch} operation.
In this case, each \emph{state} object contains a single quantity: the stretch factor $f$.
\Cref{alg:timestate} illustrates the state-generation logic for a randomized
time-stretcher, in which some $n$ examples are generated by sampling $f$ uniformly at
random over an interval $[f_-, f_+]$.  While this example is quite
simple, it serves to illustrate the power and flexibility of the architecture.  We note
that this is just one example: one could easily imagine sequential state generators that
sweep out a deterministic parameter grid, or perform otherwise arbitrary computation.

Once a state $S$ has been generated, the \emph{audio} deformation method
$D.\text{audio}(S, J.\text{audio})$
would apply the time-stretch to the audio signal, which is stored within the
JAMS sandbox upon instantiation.\footnote{The \emph{sandbox} provides unstructured
storage space within a JAMS object, which is used in our framework as a scratch space for
audio signals.}  Similarly, track-level meta-data can be modified by the \emph{metadata}
method.  In this example, time-stretching will change the track length, which is recorded
in the JAMS meta-data field.

Next, a generic \emph{annotation} deformation would be registered to the pattern
``\texttt{.*}'' and apply the stretch factor to all \emph{time} and \emph{duration}
fields of all annotations.  This deformation would leave the annotation
\emph{values} untouched.

Finally, any annotations whose \emph{value} fields depend on time, such as \emph{tempo}
can be modified directly by registering the transformation function against the
appropriate type pattern, \eg, ``\texttt{tempo}''.  Other time-dependent type
deformations would be registered separately as needed.

\begin{algorithm}[t]
    \caption{Randomized time-stretch state generator\label{alg:timestate}}
    \begin{algorithmic}[1]
        \Require{Number of deformations $n$, range bounds $(f_-, f_+)$}
        \Ensure{Sequence of states $S$}
        \Function{RandomStretch.states}{$n, f_-, f_+$}
            \For{$i$ in $1, 2, \dots, n$}
            \State{Sample $f \sim { }_U[f_-, f_+]$}
            \State{\textbf{yield} $S=\{f\}$}
            \EndFor{}
        \EndFunction{}
    \end{algorithmic}
\end{algorithm}

\subsection{Pipelines and bypasses}
\Cref{alg:transformation} describes the process by which a deformation object turns a
single annotated audio example into a sequence of deformed examples.  If we were
interested in experimenting with only a single type of augmentation (\eg, time stretching),
this would suffice.  However, some applications may require combining or cascading
multiple types of deformation, and we would prefer a unified interface that obviates the
need for customized data augmentation scripts.

Here, we draw inspiration from SciKit-Learn in defining \emph{pipeline}
objects.  The general idea is simple: two or more deformation objects can be chained
together, and treated as a single, integrated deformation object.  This functionality
mimics the SciKit-Learn \emph{Transformer} pipeline\footnote{\url{http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html}} functionality, except that rather
than applying a composition of one-to-one mappings, deformation pipelines
compose functions in a one-to-many fashion.  More precisely, for a deformation pipeline
$P$ composed of stages $D_i$:
\[
P = (D_1, D_2, \dots, D_k),
\]examples are generated by a depth-first traversal of the
cartesian product of the state spaces
${\Sigma_1 \times \Sigma_2 \times \cdots \times \Sigma_k}$, where $\Sigma_i$ corresponds
to the state space of $D_i$.  Consequently, one input example produces $n_p = \prod_{i=1}^k
|\Sigma_i|$ output examples.  However, by using generators rather than returning full
lists of states, we ensure that only $k$ examples are ever in memory at any time.
In most cases, $k$ is much smaller than $n_p$, and the improvement in memory efficiency
is substantial.

Finally, we introduce a \emph{bypass} deformation object, which can be used to mark
individual deformation objects as optional in a pipeline.  This can be useful when it is
otherwise difficult to encode a special \emph{no-op} state within a deformation object,
such as the randomized time-stretch example listed as \Cref{alg:timestate}.  Bypasses
can be used to ensure that the original examples are propagated through the
pipeline unscathed, and the resulting augmented data set is a strict superset of the
clean data.

\subsection{Reproducibility and data provenance}
\label{sec:reproducibility}
When modifying data for statistical modeling purposes, maintaining transparency is of
utmost importance to ensure reproducibility and accurate interpretation of results.
This ultimately becomes a question of data provenance~\cite{buneman2000data}: a record of
all transformations should be kept, preferably attached as closely as possible to the data.
This is accomplished at line~9 of \Cref{alg:transformation} by embedding the
\emph{state} object $S$ (and, in practice, the parameters used to construct the
deformation object $D$) within the JAMS object after each deformation is applied.
Each object generated at line~10 thus contains a full transactional history of all
modifications required to transform the original input $J$ into the output $J'$.

In addition to facilitating reproducibility, maintaining transformation provenance allows
practitioners to generate a wide range of deformations up front, and then later filter the
resulting objects.  As a result, it is straightforward to generate a suite of experiments
over different subsets of data augmentation parameters.

To further facilitate reproducibility and sharing of experimental designs, the proposed
architecture supports \emph{serialization} of deformation pipelines into a simple,
human-readable JavaScript object notation (JSON) format.
Once a pipeline has been constructed, it can be exported to
disk, edited as plain text, shared, and reloaded.  This feature also simplifies the
process of applying several different sets of deformation parameters, and eliminates the
need for writing a custom script for each setting.

\section{Example application: multi-instrument recognition}

To illustrate the data augmentation process, we applied it to the task of instrument
recognition in mixed audio signals.  For this task, we used the MedleyDB dataset, which
consists of 122 tracks, spanning a variety of genres and
instrumentations~\cite{bittner2014medleydb}.  Each track is annotated with time-varying
instrument activation functions, which are thresholded to provide a set of active
instruments at each point in time.
Because of the relatively small sample size, we limit
the experiment to cover only the 15 most frequently used (by track) instruments, as
listed in \Cref{medleytags}.

\begin{table}
\caption{The 15 most frequently used instrument labels in MedleyDB.\label{medleytags}}
\centering
\small
    \begin{tabular}{lr}
        \toprule
        Instrument & Number of tracks\\
        \midrule
        drum set                    & 65\\
        electric bass               & 64\\
        piano                       & 42 \\
        male singer                 & 38 \\
        clean electric guitar       & 37\\
        vocalists                   & 27\\
        synthesizer                 & 27\\
        female singer               & 25\\
        acoustic guitar             & 24\\
        distorted electric guitar   & 21\\
        auxiliary percussion        & 18\\
        double bass                 & 16\\
        violin                      & 14\\
        cello                       & 11\\
        flute                       & 11\\
        \bottomrule
    \end{tabular}
\end{table}

For evaluation purposes, each track is split into one-second clips.
The system is then tasked with recognizing the instruments active within each clip.

\subsection{Data augmentation}

The data augmentation pipeline consists of four stages:

\begin{description}
    \item[Pitch shift] by $n \in \{-1, 0, +1\}$ semitones.
    \item[Time stretch] by a factor of $f \in \left\{ 2^{-1/2}, 1.0, 2^{1/2}\right\}$.
    \item[Background noise (bypass)] under three conditions:
        subway,\footnote{\url{https://www.freesound.org/people/jobro/sounds/112252/}}
        crowded concert hall,\footnote{\url{https://www.freesound.org/people/klankbeeld/sounds/171317/}}
        and night-time city noise.\footnote{\url{https://www.freesound.org/people/inkhorn/sounds/231870/}}
        The latter three were linearly mixed with random weights drawn uniformly
        $\alpha \sim { }_U[0.1, 0.4]$.
    \item[Dynamic range compression (bypass)] under two settings drawn from the {Dolby E}
        standards~\cite{dolbyE}: \emph{speech},
        and \emph{music~(standard)}.
\end{description}

Pitch-shift and time-stretch operations were performed by Rubberband~\cite{rubberband}, and dynamic range
compression was performed by sox~\cite{sox}.
Note that the first two stages include no-op parameter settings $n=0$ and $f=1$.  We use
bypasses on the final two stages to ensure that all combinations of augmentation are
present in the final set.

Combining all stages of the pipeline produces {$3\times 3\times 4\times 3 = 108$} variants of each input track.  To
simplify the experiments, we only compare the cumulative effects of the above
augmentations.  This results in five training conditions of increasing complexity:
\begin{enumerate}
    \item No augmentation;
        \vspace{-.5\baselineskip}
    \item Pitch shift;
        \vspace{-.5\baselineskip}
    \item Pitch shift and time stretch;
        \vspace{-.5\baselineskip}
    \item Pitch shift, time stretch, and background noise;
        \vspace{-.5\baselineskip}
    \item All stages.
\end{enumerate}

\subsection{Acoustic model}

% Input features:
%   CQT at 36 bpo, ranging from C3 to C8 => 216 bins, 512-frame hop at 22KHz
%   sample patches of 44 frames ~= 1.02s
%   logamplitude clipped to -80dB
%
%   CQT features enable 2d-convolution for pitch-invariant feature extraction
%   log scaling gives us relative amplitude invariance
%

The acoustic model used in these experiments is a deep convolutional network.
The input to the network consists of log-amplitude, constant-Q spectrogram patches extracted with librosa~\cite{librosa}.
Each example spans approximately one second of audio, corresponding to 44 frames at a hop length of 512 samples and sampling rate of $22050$~Hz.
Constant-Q spectrograms cover the range of C3 ($65.41$~Hz) to C9 ($4186$~Hz) at 36 bins per octave, resulting in time-frequency patches of size $216\times44$.
Instrument activations are aggregated into a single binary label vector, such that an instrument is deemed active if its on-time within the sample exceeds 0.25 seconds.

Importantly, constant-Q representations are linear in both time and pitch, a property that can be exploited by convolutional neural networks to achieve translation invariance.
Thus a four-layer model is designed to estimate the presence of zero or more instruments in a time-frequency patch.
Formally, an input, $X$, is transformed into an output, $Z$, via a composite nonlinear function $\mathcal{F}(\cdot \vert \Theta)$, given a parameter set $\Theta$.
This is achieved as a sequential cascade of $L=4$ operations, $f_l(\cdot \vert \theta_l)$, referred to as \emph{layers}, the order of which is given by $l$:

\begin{equation}
\label{eq:layers}
Z = \mathcal{F}(X \vert \Theta) = f_{L}(  ... f_2(f_1(X \vert \theta_1) \vert \theta_2) ) ... \vert \theta_{L})
\end{equation}

% \noindent Here, $\mathcal{F} = \{f_1, f_2, ... f_{L} \}$ is the set of layers, $\Theta = \{\theta_1, \theta_2, ... \theta_{L} \}$, the corresponding set of parameters, and the output of one layer is passed as the input to the next, as $X_{l+1} = Z_{l}$.

The first two layers, $l \in \{1, 2\}$, are convolutional, expressed by the following:

\begin{equation}
\label{eq:convlayer}
\small
Z_l = f_l(X_l \vert \theta_l) = h(X_{l} \circledast W + b), \quad \theta_l = [W, b]
\end{equation}

\noindent Here, the valid convolution, $\circledast$, is computed by convolving a 3D input tensor, $X_l$, consisting of $N$ \emph{feature maps}, with a collection of $M$ 3D-\emph{kernels}, $W$, followed by an additive vector bias term, $b$, and transformed by a point-wise activation function, $h(\cdot)$.
In this formulation, $X_l$ has shape $(N, d_0, d_1)$, $W$ has shape $(M, N, m_0, m_1)$, and the output, $Z_l$, has shape $(M, d_0-m_0+1, d_1-m_1+1)$.
% Note that the number of feature maps, $N$, must match between input and kernels.
Max-pooling is applied in time and frequency, to further accelerate computation by reducing the size of feature maps, and allowing a small degree of scale invariance in both time and pitch.

The final two layers, $l \in \{3, 4\}$, are fully-connected matrix products, given as follows:

\begin{equation}
\label{eq:fclayer}
Z_l = f_l(X_l \vert \theta_l) = h( W~X_{l} + b), \quad \theta_l = [W, b]
\end{equation}

\noindent Here, the input to the $l^{th}$ layer, $X_l$, is flattened to a column vector of length $N$, projected against a weight matrix, $W$, of shape $(M, N)$, added to a vector bias term, $b$, of length $M$, and transformed by a point-wise activation function, $h(\cdot)$.

The network is parameterized thusly:
$l_1$ uses $W$ with shape $(24, 1, 13, 9)$, $(2, 2)$ max-pooling over the last two dimensions, and a rectified linear (ReLU) activation function, \ie $\max(x, 0)$;
$l_2$ uses $W$ with shape $(48, 24, 9, 7)$, $(2, 2)$ max-pooling over the last two dimensions, and a ReLU activation function;
$l_3$ uses $W$ with shape $(17280, 96)$ and a ReLU activation function;
finally, $l_4$ uses $W$ with shape $(96, 15)$ and a sigmoid activation function.

During training, the model is optimized to a cross-entropy loss via mini-batch stochastic gradient descent, $n=50$, with a constant learning rate of 0.02.
Dropout is applied to the activations of the penultimate layer, $l=3$, with $p=0.5$, and a small amount of $l_2$-regularization is applied to the weights of the final layer, $l=4$, to prevent the weights from growing arbitrarily large and causing numerical instability.


\subsection{Experimental procedure and results}

% Repeat x5:
%   randomly partition artists
%   train, validate, test for each model
%   compare per-track meanAP of tags
Five random artist-conditional partitions of the MedleyDB collection were generated with
a train/test artist ratio of 4:1.  For the purposes of this experiment,
the ``artist'' \emph{MusicDelta} was separated into a collection of distinct pseudo-artists
depending on the genres of the recordings.
For each train/test split of the data, the training set was further partitioned into
training and validation sets, again at a ratio of 4:1.
To evaluate performance, we compute for each test track the mean label-ranking average
precision (LRAP) of predictions over all one-second patches in the track.  We opted for
a ranking-based metric, rather than classification metrics (\eg, F1-score) because the
latter can be sensitive to class bias and the choice of detection thresholds.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figs/lrap}
    \caption{Test-set score distributions (label-ranking average precision) for each
        randomized partition of the data.
        Augmentation conditions are indexed as: 1 (no augmentation), 2 (pitch-shift), 3
        (pitch and time), 4 (pitch, time, and noise), and 5 (pitch, time, noise, and
        DRC).\label{lrapresults}}
\end{figure*}

\Cref{lrapresults} illustrates the test-set performance for each of the train/test
splits.  Because of the relatively small size of MedleyDB and the need for
artist-conditional splits, the resulting test-sets are small and thus exhibit high
variance across splits.  However, a few trends can be readily observed from the figure.
First, the models trained with data augmentation (2--5) all generalize at least as well
as the one trained only on clean data (1).  This indicates that the learning algorithm is
not corrupted by training on noisy examples.

\begin{figure}
    \centering
    \includegraphics[width=\columnwidth]{figs/onevstwo}
    \caption{Point-wise comparison of test-set scores (LRAP)
        for augmentation condition 1 (no augmentation) vs.\ condition 2 (pitch-shifting).
        Each data point corresponds to a single test track, and is colored according to
        the random train-test split from which it was drawn.\label{onevstwo}}
\end{figure}

To more clearly investigate the effects of pitch-shifting augmentation, \Cref{onevstwo}
illustrates the differences in accuracy between the no-augmentation models (1) and the
models trained with additional pitch-shifted data (2), aggregated across all splits.
The majority of test points lie above the main diagonal, indicating
consistent improvements due to data augmentation.

\section{Conclusion}

% For bibtex users:
\bibliography{refs}

\end{document}
