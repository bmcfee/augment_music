% -----------------------------------------------
% Template for ISMIR Papers
% 2015 version, based on previous ISMIR templates
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir}
\usepackage{url}
\usepackage{cleveref}
\usepackage{cite}
\usepackage{brian}
\usepackage{graphicx}
\usepackage{booktabs}

% Title.
% ------
\title{Pump up the jams: musical data augmentation}


% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Predictive models for music annotation tasks are practically limited by a paucity of
well-annotated training data.
In this work, we develop a general framework for augmenting annotated musical datasets,
allowing practitioners to expand training sets in a controlled fashion.
We investigate the effects of data augmentation on the task of recognizing instruments
in mixed signals.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}

% Main points to make:
%   1. music is complex, and needs big models
%       a. big models need big data
%       b. big (annotated) data is hard to come by

Musical audio signals contain a wealth of rich, complex, and highly structured
information.  The primary goal of content-based music information retrieval is to
analyze, extract, and summarize music recordings in a human-friendly
format, such as semantic tags, chord and melody annotations, or structural boundary
estimations.  To adequately capture and characterize the vast complexity of musical
recordings seems to require large, flexible models.  In short, complex data necessitate
complex models.
By the same token, estimating the parameters of complex statistical models often requires
a large number of samples: big models require big data.

Within the past few years, a similar trend has played out in the computer vision
literature.  Currently, the best-performing models for recognition of objects in images
exploit two fundamental properties: access to large quantities of annotated data, and
identification of label-invariant transformations~\cite{krizhevsky2012imagenet}.
The benefits of large training collections are obvious, and unfortunately, difficult to 
carry over to most musical annotation tasks due to the complexity of the label space and
need for expert annotators.

%   2. in images: 
%       a. data augmentation has proven useful in vision. 
%       b. general idea: perturb your data such that the features change, but the labels
%       don't
%       c. rotations, reflections, contrast normalization, affine transformations

\cite{krizhevsky2012imagenet}

%   3. but data augmentation is more difficult in music than in images
%       a. the output space is much more complex than simple tags
%       b. it's not obvious which variations preserve output structure
%       c. simple example: time-stretching will move annotation boundaries
%       d. complex example: pitch-shifting will deform chord labels

\subsection{Our contributions}
% 1. develop a generic framework for synchronously manipulating audio and annotations
% 2. investigate the effects of simple deformations on the problem of musical instrument recognition

\section{Related work}

% 1. it's common to engineer systems to attempt to resolve symmetries in the input,
%       eg, chroma features are engineered to be approximately invariant to timbre and octave
\cite{muller2011chroma}
%       some authors suppress the 0th mfcc to get loudness invariance
\cite{pampalk2004matlab}

%   but there are some drawbacks:
%       a. it's not easy to identify *all* relevant symmetries
%       b. even if it was, it might not be easy to engineer an invariant feature
%       c. and you might accidentally discard useful information in the process
%
%   alternatively: 
%       we can use bigger models
%       learn the appropriate invariances from statistics.
%       but this takes a lot of (annotated) data, which we usually don't have
%   
%

% 1. augmentation is not new, but it hasn't been done systematically.
%   eg, chroma rotation for key-invariance in chord quality or mode
%   synthetic mixtures of clean signals
%   
\cite{ellis2007classifying}
\cite{kirchhoff2012multi}

% finally, recent studies have investigated stability of models by evaluating on degraded signals:
%   but it's not clear that the degraded signals resemble the distribution of naturally occurring sounds
%   our goal is different: train on degraded signals, and evaluate on unmodified signals
%   
%   
\cite{mauch2013audio}
\cite{sturmkiki}

\section{Data augmentation architecture}

% 1. because of the complex structure of annotations, we need to be careful
%   a. annotations aren't just track-level, but generally time-keyed
%   b. simple deformations can change annotations, such as chord labels or pitch
%   frequencies
%
% 2. why not use the audio degradation toolbox?
%   a. we'd like to be more extensible, support annotation-dependent deformations
%   b. support multiple annotations per-track
%   c. want the ability to embed history within the annotations for reproducibility
%   purposes
%   d. closer integration with python libraries for machine learning (eg theano)
%
% 3. we developed a generic, plugin-oriented architecture for doing musical data augmentation
%   a. hooks into JAMS, and inherits validation/schema.  
%   b. also makes it easy to modify all annotations for a given track in one shot.
%   c. allows the developer to register a deformation against different types of data
%       eg, a ``pitch-shift'' deformer implements an audio deformation, pitch annotation modification, and
%       chord/key manipulators
%   e. modules can be chained or skipped in a pipeline, similar to sklearn feature extractors
%       pipelines can be serialized, stored, shared, and reimplemented easily
%   f. full history of modification state is preserved within the output JAMS sandbox, so the results are
%   documented and reproducible
%       this includes all random state
%   g. deformations can be stochastic, and potentially generate infinite streams of randomized data
%       more generally, a deformer object can implement its own state transition logic
%       using python iterators makes this simple, self-contained, and memory-efficient

\cite{humphreyjams}

% Give a figure illustrating how deformers work
%   deformer 
%       generates states as a function of a jam
%       registers callbacks against annotation types
%   pipeline
%       chains deformers together
%   bypass
%       make a deformer optional
\subsection{Audio deformation}

\subsection{Annotation deformations}

\subsection{Pipelines}

\section{Example application: multi-instrument recognition}

\cite{bittner2014medleydb}

\subsection{Data augmentation}

The data augmentation pipeline consists of four stages:

\begin{description}
    \item[Pitch shift] by $n \in \{-1, 0, +1\}$ semitones.
    \item[Time stretch] by a factor of $f \in \left\{ 2^{-1/2}, 1.0, 2^{1/2}\right\}$.
    \item[Background noise] under four conditions: no noise,
        subway,\footnote{\url{https://www.freesound.org/people/jobro/sounds/112252/}}
        crowded concert hall,\footnote{\url{https://www.freesound.org/people/klankbeeld/sounds/171317/}}
        and night-time city noise.\footnote{\url{https://www.freesound.org/people/inkhorn/sounds/231870/}}
        The latter three were mixed with random weights drawn uniformly in $[0.1, 0.4]$.
    \item[Dynamic range compression] under three preset conditions drawn from the {Dolby E}
        standards~\cite{dolbyE}: none, \emph{speech},
        and \emph{music (standard)}.
\end{description}

Pitch-shift and time-stretch operations were performed by Rubberband~\cite{rubberband}, and dynamic range
compression was performed by sox~\cite{sox}.

Combining all stages of the pipeline produces {$3\times 3\times 4\times 3 = 108$} variants of each input track.  To
simplify the experiments, we only compare the cumulative effects of the above
augmentations.  This results in five training conditions of increasing complexity:
\begin{enumerate}
    \item No augmentation;
        \vspace{-.5\baselineskip}
    \item Pitch shift;
        \vspace{-.5\baselineskip}
    \item Pitch shift and time stretch;
        \vspace{-.5\baselineskip}
    \item Pitch shift, time stretch, and background noise;
        \vspace{-.5\baselineskip}
    \item All stages.
\end{enumerate}

\subsection{Acoustic model}

% Input features:
%   CQT at 36 bpo, ranging from C3 to C8 => 216 bins, 512-frame hop at 22KHz
%   sample patches of 44 frames ~= 1.02s
%   logamplitude clipped to -80dB
%
%   CQT features enable 2d-convolution for pitch-invariant feature extraction
%   log scaling gives us relative amplitude invariance
%
% convolutional model:
%   input: 216 x 44
%   layer 1: 24 filters of shape (13x9)
%       output dimension: 24 x (216 - 2*13+1) x (44 - 2 * 9 + 1)
%       relu
%   layer 2: 48 filters of shape (24x9x7)
%       relu
%   downsample 2x2 max-pooling
%   dense layer of size 96
%       relu
%       dropout = 0.5
%   dense layer for output
%       sigmoid nonlinearity
%       weight decay
%   output: n_classes

\cite{librosa}

\subsection{Results}

% Repeat x5:
%   randomly partition artists
%   train, validate, test for each model
%   compare per-track meanAP of tags

\cite{scikit-learn}

\section{Conclusion}

% For bibtex users:
\bibliography{refs}

\end{document}
