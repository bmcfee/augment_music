% -----------------------------------------------
% Template for ISMIR Papers
% 2015 version, based on previous ISMIR templates
% -----------------------------------------------

\documentclass{article}
\usepackage{ismir}
\usepackage{url}
\usepackage{cleveref}
\usepackage{cite}
\usepackage{brian}
\usepackage{graphicx}
\usepackage{booktabs}

% Title.
% ------
\title{Pump up the jams: musical data augmentation}


% Single address
% To use with only one author or several with the same address
% ---------------
%\oneauthor
% {Names should be omitted for double-blind reviewing}
% {Affiliations should be omitted for double-blind reviewing}

% Two addresses
% --------------
%\twoauthors
%  {First author} {School \\ Department}
%  {Second author} {Company \\ Address}

% Three addresses
% --------------
\threeauthors
  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
  {Second author} {\bf Retain these fake authors in\\\bf submission to preserve the formatting}
  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}

% Four addresses
% --------------
%\fourauthors
%  {First author} {Affiliation1 \\ {\tt author1@ismir.edu}}
%  {Second author}{Affiliation2 \\ {\tt author2@ismir.edu}}
%  {Third author} {Affiliation3 \\ {\tt author3@ismir.edu}}
%  {Fourth author} {Affiliation4 \\ {\tt author4@ismir.edu}}

\begin{document}
%
\maketitle
%
\begin{abstract}
Predictive models for music annotation tasks are practically limited by a paucity of
well-annotated training data.
In this work, we develop a general framework for augmenting annotated musical datasets,
allowing practitioners to expand training sets in a controlled fashion.
We investigate the effects of data augmentation on the task of recognizing instruments
in mixed signals.
\end{abstract}
%
\section{Introduction}
\label{sec:introduction}

% Main points to make:
%   1. music is complex, and needs big models
%       a. big models need big data
%       b. big (annotated) data is hard to come by

Musical audio signals contain a wealth of rich, complex, and highly structured
information.  The primary goal of content-based music information retrieval (MIR) is to
analyze, extract, and summarize music recordings in a human-friendly
format, such as semantic tags, chord and melody annotations, or structural boundary
estimations.  To adequately capture and characterize the vast complexity of musical
recordings seems to require large, flexible models.  In short, complex data necessitate
complex models.
By the same token, estimating the parameters of complex statistical models often requires
a large number of samples: big models require big data.

Within the past few years, this same phenomenon of model complexity has been observed 
in the computer vision literature.  Currently, the best-performing models for recognition 
of objects in images exploit two fundamental properties to overcome the difficulty of 
fitting large, complex models: access to large quantities of annotated data, and 
identification of label-invariant transformations~\cite{krizhevsky2012imagenet}.
The benefits of large training collections are obvious, and unfortunately, difficult to 
carry over to most musical annotation tasks due to the complexity of the label space and
need for expert annotators.  However, the idea of generating perturbations of a training
set --- known as \emph{data augmentation} --- can be readily adapted to musical tasks.

%   2. in images: 
%       a. data augmentation has proven useful in vision. 
%       b. general idea: perturb your data such that the features change, but the labels
%       don't
%       c. rotations, reflections, contrast normalization, affine transformations
Conceptually, data augmentation consists of the application of one or more deformations to
a collection of (annotated) training samples.  
The motivation for data augmentation is that a learning algorithm should be less
susceptible to spurious correlations and over-fitting if it is provided with many
observations of an instance which have been perturbed in ways which do not affect its
label.
Some concrete examples of deformations drawn from computer vision include translation, 
rotations, and reflections.  These simple operations are appealing because they typically 
do not affect the target class label. An upside-down image of a cat is still contains cat, 
although the situation may be more complex for concepts which are not reflection-invariant, 
such as in optical character recognition.  Consequently, practitioners must exercise some 
caution when applying data augmentation techniques to ensure that the correct invariances 
are maintained.

Continuing with the image example, if an image is rotated, then any pixel-wise 
label annotations (\eg, bounding boxes) should be modified accordingly.  
More generally, deformations apply not only to the observable stimulus, 
but its annotations as well.
This observation opens up several interesting possibilities for musical applications, in
which the target concept space typically exhibits a high degree of structure.
As a simple example, time-stretching an audio track should also result in movement of 
time-keyed annotations~\cite{mauch2013audio}.  
Moreover, time-dependent annotation \emph{values} such as \emph{tempo} can also be adjusted.
As a more complex example,
pitch-shifting a track may (or may not) result in transpositions of pitch contours,
chord labels, or symbolic annotations.
It follows that successfully applying data augmentation techniques in MIR may require
substantially more careful implementations and transformation algorithms than a currently 
in use in other domains.

%   3. but data augmentation is more difficult in music than in images
%       a. the output space is much more complex than simple tags
%       b. it's not obvious which variations preserve output structure
%       c. simple example: time-stretching will move annotation boundaries
%       d. complex example: pitch-shifting will deform chord labels

\subsection{Our contributions}
% 1. develop a generic framework for synchronously manipulating audio and annotations
% 2. investigate the effects of simple deformations on the problem of musical instrument recognition
In this work, we describe a software architecture for applying data augmentation to music
information retrieval tasks.\footnote{For anonymity purposes, the name of the software
    described here is redacted throughout the text.  This will be changed in the final
draft, and download links will be provided.}
The system is designed to be simple, modular, and
extensible. The design enables practitioners to develop custom deformations, and combine 
multiple simple deformations together into pipelines which can generate large volumes of 
reliably deformed, annotated music data.  The proposed system is built on top of
JAMS~\cite{humphreyjams}, which provides a simple container for accessing and
transporting multiple annotations for a given track.  

As a simple proof of concept, we apply the proposed data augmentation architecture to the
task of recognizing instruments in mixed signals, and demonstrate that even simple
manipulations can provide substantial improvements in model performance.

\section{Related work}

% 1. it's common to engineer systems to attempt to resolve symmetries in the input,
%       eg, chroma features are engineered to be approximately invariant to timbre and octave
%       some authors suppress the 0th mfcc to get loudness invariance
As practitioners, the first step in developing a solution to some MIR task is often to 
design features which discard information which is irrelevant to predicting the target
concept.  For example, chroma features are designed to capture pitch class information
and suppress information derived from timbre, loudness, or octave 
height~\cite{muller2011chroma}.  Similarly, many authors interested in modeling timbre
use Mel-frequency cepstral coefficients (MFCCs) and discard the first component to
achieve invariance to loudness~\cite{pampalk2004matlab}.
While this general strategy makes intuitive, practical sense, it carries certain
limitations.  First, it is not necessarily easy to identify all relevant symmetries the
data: if it was easy, the problem would be essentially solved.  Second, even if such
properties are easy to identify, it may still be difficult to engineer invariant
features.  Finally, even if one does succeed in designing appropriately invariant
features, they may inadvertently discard relevant information in the process.

%   but there are some drawbacks:
%       a. it's not easy to identify *all* relevant symmetries
%       b. even if it was, it might not be easy to engineer an invariant feature
%       c. and you might accidentally discard useful information in the process
%
%   alternatively: 
%       we can use bigger models
%       learn the appropriate invariances from statistics.
%       but this takes a lot of (annotated) data, which we usually don't have
%   
%
As an alternative to custom feature design, some authors advocate learning or optimizing
features directly from the data~\cite{humphrey2012moving}.  
Perhaps not surprisingly, this approach typically requires large model architectures, and 
consequently requires much larger (annotated) data sets than had previously been 
used in MIR research.  
Our goal in this work is to ease the burden of sample complexity, and make 
data-driven models more accessible to the MIR community.

% 1. augmentation is not new, but it hasn't been done systematically.
%   eg, chroma rotation for key-invariance in chord quality or mode
%   synthetic mixtures of clean signals
%   perturbations of the labels ``target smearing''
%
Specific instances of data augmentation can be found throughout the MIR literature,
though they are not often identified as such, nor are they treated in a unified,
systematic way.  For example, it is common to apply circular rotations to chroma features 
in order to achieve key invariance when modeling chord quality~\cite{lee2008acoustic}.
Alternately, synthetic mixtures of monophonic instruments have been used 
to train polyphonic transcription engines~\cite{kirchhoff2012multi}.
At the other end of the spectrum, some authors leave the content unchanged and 
only modify labels during training, as exemplified by the \emph{target smearing} 
method of Ullrich~\etal\ for training structural boundary 
detectors~\cite{ullrich2014boundary}.

% finally, recent studies have investigated stability of models by evaluating on degraded signals:
%   but it's not clear that the degraded signals resemble the distribution of naturally occurring sounds
%   our goal is different: train on degraded signals, and evaluate on unmodified signals
%   
%   
Finally, recent studies have used degraded signals to evaluate the stability of
existing methods for MIR tasks.
The Audio Degradation Toolbox (ADT) was developed for this purpose, and was used
to measure the impact of naturalistic deformations of audio on several tasks, including
beat tracking, score alignment, and chord recognition~\cite{mauch2013audio}.
Similarly, Sturm and Collins proposed the ``Kiki-Bouba Challenge'' as a way to determine
whether statistical models of musical concepts actually capture the defining
characteristics of a category (\eg, genre), or are over-fitting to spurious
correlations~\cite{sturmkiki}.

In both of the studies cited above, models are fit to unmodified data, and evaluated in
degraded conditions under the control of the experimenter.  
Data augmentation provides the converse of this setting: models are fit to degraded data, 
and evaluated on unmodified examples.  The distinction between the two approaches is
critical.  The former attempts to measure the robustness of a system under synthetic
conditions, while the latter attempts to improve robustness by \emph{training} under
synthetic conditions. Note that with data augmentation, the evaluation set is outside 
the control of the experimenter, so the resulting comparisons are unbiased with respect
to the underlying distribution from which the data are sampled.  While this does not
directly measure robustness of the resulting system, it has still been observed that data
augmentation can be used effectively to improve generalization in
practice~\cite{krizhevsky2012imagenet}.


\section{Data augmentation architecture}

% 1. because of the complex structure of annotations, we need to be careful
%   a. annotations aren't just track-level, but generally time-keyed
%   b. simple deformations can change annotations, such as chord labels or pitch
%   frequencies
%
% 2. why not use the audio degradation toolbox?
%   a. we'd like to be more extensible, support annotation-dependent deformations
%   b. support multiple annotations per-track
%   c. want the ability to embed history within the annotations for reproducibility
%   purposes
%   d. closer integration with python libraries for machine learning (eg theano)
%
% 3. we developed a generic, plugin-oriented architecture for doing musical data augmentation
%   a. hooks into JAMS, and inherits validation/schema.  
%   b. also makes it easy to modify all annotations for a given track in one shot.
%   c. allows the developer to register a deformation against different types of data
%       eg, a ``pitch-shift'' deformer implements an audio deformation, pitch annotation modification, and
%       chord/key manipulators
%   e. modules can be chained or skipped in a pipeline, similar to sklearn feature extractors
%       pipelines can be serialized, stored, shared, and reimplemented easily
%   f. full history of modification state is preserved within the output JAMS sandbox, so the results are
%   documented and reproducible
%       this includes all random state
%   g. deformations can be stochastic, and potentially generate infinite streams of randomized data
%       more generally, a deformer object can implement its own state transition logic
%       using python iterators makes this simple, self-contained, and memory-efficient
Our implementation takes inspiration from the Audio Degradation
Toolbox~\cite{mauch2013audio}.  In principle, the ADT can be used directly 
for data augmentation simply by applying it to the training set rather than test set.
However, we opted for an independent, Python-based implementation for a variety of
reasons as detailed below.

% 1. object-oriented design
%   simple, reusable components that are easy to modify and extend
%   
First, Python enables object-oriented design, which in turn allows for structured,
extensible, and reusable code.  This in turn facilitates a simple interface shared across
all \emph{deformation objects}, and makes it easy for practitioners to combine existing
deformations, or implement new transformations with minimal effort.

% 2. leverage jams as a container
%   transform all annotations for an object at once
%       -> everything stays in sync easily
%   store the transformation properties within the jams sandbox
%       -> reproducibility, provenance
%   the jams sandbox can store the audio and other intermediate features
%       -> 
%   allow deformers to apply different transformations to different types of annotations
%       implemented as simple callbacks registered against annotation types or patterns
%

Second, we leverage JAMS objects~\cite{humphreyjams} to contain and transport all
annotations for a given track.
This simplifies the task of maintaining synchronization between audio and annotations,
and also facilitates task-dependent annotation deformations.  Moreover, we can take
advantage of JAMS meta-data fields to provide data provenance and facilitate
reproducibility.

% 3. use familiar design paradigms
%   take inspiration from the Transformer and Pipeline paradigms of scikit-learn
%
Finally, we can borrow familiar software design concepts from the SciKit-Learn
package~\cite{scikit-learn}, such as the notions of \emph{transformers},
\emph{pipelines}, and model serialization.  These simple building blocks allow
practitioners to develop and deploy complex data augmentation pipelines from small,
conceptually simple components.

% Give a figure illustrating how deformers work
%   deformer 
%       generates states as a function of a jam
%       registers callbacks against annotation types
%   pipeline
%       chains deformers together
%   bypass
%       make a deformer optional

In the remainder of this section, we will describe each of these
properties in more detail.  Without loss of generality, we will assume that an
annotation (\eg, instrument activations or semantic tags) is encoded as a collection of
tuples: \emph{(time, duration, value, confidence)}.  
Note that instantaneous events can be represented as having zero duration, while
track-level annotations have full-track duration.  The \emph{value} field will depend on
the annotation type, and may encode strings, numeric quantities, or fully structured
objects.

\subsection{Deformation objects}

At the core of our implementation is the concept of a \emph{deformation object}.
A deformation object implements one or more \emph{transformation} methods, each of which 
applies to audio data, meta-data, or annotations.  Since each of these methods are
self-contained, a \emph{state} object is used to share information across method calls,
so each transformation method takes as input a state-data pair $(S, x)$ and returns the 
suitably transformed audio, meta-data, or annotation $x'$.

It would be inefficient to use a separate deformation object for each parameterization of
a deformation.  In practice, we would generally prefer to iterate over a range of
parameters, or randomly sample from a distribution of deformations.  To support this
behavior, deformation objects can implement arbitrary state transition logic, and produce
a sequence of state objects through Python's \emph{generator} functionality.

The high-level transformation engine is described in
\Cref{alg:transformation}.  For each state $S$, the input data is copied, transformed,
and yielded.  Each transformed object $J'$ can either be saved off to disk, fed directly
as a sample to a learning algorithm, or passed along to another deformation object in a
pipeline for subsequent processing.

\begin{algorithm}[t]
\caption{Abstract transformation pseudocode\label{alg:transformation}}
\begin{algorithmic}[1]
    \Require{Deformation object $D$, JAMS object $J$}
    \Ensure{Sequence of transformed JAMS objects $J'$}
    \Function{$D$.transform}{$J$}
    \For{states $S$ in $D$}
        \State{$J' \leftarrow \text{copy}(J)$}
        \State{$J'$.audio $\leftarrow D$.audio($S$, $J'$.audio)}
        \State{$J'$.metadata $\leftarrow D$.metadata($S$, $J'$.metadata)}
        \For{transformations $g$ in $D$}
            \For{annotations $a$ in $J'$ which match $g$}
                \State{$J'.a \leftarrow g(S, a)$}
            \EndFor{}
        \EndFor{}
        \State{$J'$.history $\leftarrow S + J'$.history \label{transform:history}}
        \State{\textbf{yield} $J'$}
    \EndFor{}
    \EndFunction{}
\end{algorithmic}
\end{algorithm}


\subsubsection{Example: time-stretching}
To illustrate the deformation object interface, consider a \emph{time-stretch} operation.
In this case, each \emph{state} object contains a single quantity: the stretch factor $f$.
\Cref{alg:timestate} illustrates the state-generation logic for a randomized
time-stretcher, in which some $n$ examples are generated by sampling stretch
factors uniformly at random from some range $[f_-, f_+]$.  While this example is quite
simple, it serves to illustrate the power and flexibility of the architecture.

Once a state $S$ has been generated, the \emph{audio} deformation would apply the 
time-stretch to the audio signal, which is stored within the JAMS sandbox upon 
instantiation.\footnote{The \emph{sandbox} provides unstructured storage space within a
JAMS object, which is used in our framework as a scratch space for audio signals.}
Next, a generic \emph{annotation} deformation applies the time stretch to all
\emph{time} and \emph{duration} fields of all annotations, but leaves the \emph{values}
untouched.
Finally, any annotations whose \emph{value} fields depend on time, such as \emph{tempo}
can be modified directly.
To facilitate this functionality, deformation objects register transformation functions
against annotation type regular expressions: \eg, ``\texttt{.*}'' matches all annotations, 
while ``\texttt{chord.*}'' matches only chord-type annotations.

\begin{algorithm}[t]
    \caption{Randomized time-stretch state logic\label{alg:timestate}}
    \begin{algorithmic}[1]
        \Require{Number of deformations $n$, range bounds $(f_-, f_+)$}
        \Ensure{Sequence of states $S$}
        \Function{RandomStretch.states}{$n, f_-, f_+$}
            \For{$i$ in $1, 2, \dots, n$}
            \State{Sample $f \sim { }_U[f_-, f_+]$}
            \State{\textbf{yield} $S=\{f\}$}
            \EndFor{}
        \EndFunction{}
    \end{algorithmic}
\end{algorithm}

\subsection{Pipelines and bypasses}


\subsection{Reproducibility and data provenance}
When modifying data for statistical modeling purposes, maintaining transparency is of
utmost importance to ensure reproducibility and accurate interpretation of results.
This ultimately becomes a question of data provenance~\cite{buneman2000data}: a record of
all transformations should be kept, preferably attached as closely to the data as
possible.  This is accomplished at line~9 of \Cref{alg:transformation} by embedding the 
\emph{state} object $S$ (and, in practice, the parameters used to construct $D$) within the JAMS object after each deformation is applied.  
Each object generated at line~10 thus contains a full transactional history of all
modifications required to transform the original input $J$ into the output $J'$.

In addition to facilitating reproducibility, maintaining transformation provenance allows
practitioners to generate a wide range of deformations up front, and then later filter the
resulting objects.  As a result, it is straightforward to generate a suite of experiments
over different subsets of data augmentation parameters.

\section{Example application: multi-instrument recognition}

\cite{bittner2014medleydb}

\subsection{Data augmentation}

The data augmentation pipeline consists of four stages:

\begin{description}
    \item[Pitch shift] by $n \in \{-1, 0, +1\}$ semitones.
    \item[Time stretch] by a factor of $f \in \left\{ 2^{-1/2}, 1.0, 2^{1/2}\right\}$.
    \item[Background noise] under four conditions: no noise,
        subway,\footnote{\url{https://www.freesound.org/people/jobro/sounds/112252/}}
        crowded concert hall,\footnote{\url{https://www.freesound.org/people/klankbeeld/sounds/171317/}}
        and night-time city noise.\footnote{\url{https://www.freesound.org/people/inkhorn/sounds/231870/}}
        The latter three were linearly mixed with random weights drawn uniformly
        $\alpha \sim { }_U[0.1, 0.4]$.
    \item[Dynamic range compression] under three preset conditions drawn from the {Dolby E}
        standards~\cite{dolbyE}: none, \emph{speech},
        and \emph{music (standard)}.
\end{description}

Pitch-shift and time-stretch operations were performed by Rubberband~\cite{rubberband}, and dynamic range
compression was performed by sox~\cite{sox}.

Combining all stages of the pipeline produces {$3\times 3\times 4\times 3 = 108$} variants of each input track.  To
simplify the experiments, we only compare the cumulative effects of the above
augmentations.  This results in five training conditions of increasing complexity:
\begin{enumerate}
    \item No augmentation;
        \vspace{-.5\baselineskip}
    \item Pitch shift;
        \vspace{-.5\baselineskip}
    \item Pitch shift and time stretch;
        \vspace{-.5\baselineskip}
    \item Pitch shift, time stretch, and background noise;
        \vspace{-.5\baselineskip}
    \item All stages.
\end{enumerate}

\subsection{Acoustic model}

% Input features:
%   CQT at 36 bpo, ranging from C3 to C8 => 216 bins, 512-frame hop at 22KHz
%   sample patches of 44 frames ~= 1.02s
%   logamplitude clipped to -80dB
%
%   CQT features enable 2d-convolution for pitch-invariant feature extraction
%   log scaling gives us relative amplitude invariance
%
% convolutional model:
%   input: 216 x 44
%   layer 1: 24 filters of shape (13x9)
%       output dimension: 24 x (216 - 2*13+1) x (44 - 2 * 9 + 1)
%       relu
%   layer 2: 48 filters of shape (24x9x7)
%       relu
%   downsample 2x2 max-pooling
%   dense layer of size 96
%       relu
%       dropout = 0.5
%   dense layer for output
%       sigmoid nonlinearity
%       weight decay
%   output: n_classes

\cite{librosa}

\subsection{Results}

% Repeat x5:
%   randomly partition artists
%   train, validate, test for each model
%   compare per-track meanAP of tags

\section{Conclusion}

% For bibtex users:
\bibliography{refs}

\end{document}
